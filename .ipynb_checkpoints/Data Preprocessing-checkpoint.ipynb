{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "In this notebook I'll play with the datasets and try to convert them into sequence matrices then save these into CSV or PICKLE files. The goal is to have a Q&A like data structure both with word IDs and English words for each dataset (Twitter + Cornell Movie DB). The raw data is downloaded from https://github.com/suriyadeepan/datasets and I also use some snippets from his codes instead of using Keras' word tokenizer.\n",
    "\n",
    "## 1. Twitter Data\n",
    "\n",
    "This dataset contains questions and answers to them. All I have to do is to sort out the unnecessary data and organize the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import keras\n",
    "\n",
    "# preprocessed data\n",
    "from data.twitter import data\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, idx_q, idx_a = data.load_data(PATH='data/twitter/')\n",
    "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)\n",
    "(X, Y), _, _ = data_utils.split_dataset(idx_q, idx_a, [1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267518"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_seq = [X, Y]\n",
    "pickle.dump(twitter_seq, open('data/twitter_seq.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_lang = pd.DataFrame(columns=[\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c74f486c3f84eb68d6e8bad82b8d5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=267518), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metadata['idx2w'][1] = ''\n",
    "for idx in tqdm_notebook(range(len(X))):\n",
    "    ans = data_utils.decode(X[idx], metadata['idx2w'], ' ')\n",
    "    resp = data_utils.decode(Y[idx], metadata['idx2w'], ' ')\n",
    "    \n",
    "    twitter_lang.loc[len(twitter_lang)] = [ans, resp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_lang.to_csv('data/twitter_dialogue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(metadata, open('data/twitter_metadata.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cornell Movie Database\n",
    "\n",
    "This dataset is larger and somewhat better. I'll use the same exact method to organize it into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cornell import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = prepare_data.get_id2line()\n",
    "convs = prepare_data.get_conversations()\n",
    "questions, answers = prepare_data.gather_dataset(convs,id2line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_lang = pd.DataFrame(columns=[\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_lang['question'] = questions\n",
    "cornell_lang['answer'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
       "      <td>Forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
       "      <td>Cameron.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The thing is, Cameron -- I'm at the mercy of a...</td>\n",
       "      <td>Seems like she could get a date easy enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why?</td>\n",
       "      <td>Unsolved mystery.  She used to be really popul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gosh, if only we could find Kat a boyfriend...</td>\n",
       "      <td>Let me see what I can do.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C'esc ma tete. This is my head</td>\n",
       "      <td>Right.  See?  You're ready for the quiz.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't want to know how to say that though.  ...</td>\n",
       "      <td>That's because it's such a nice one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How is our little Find the Wench A Date plan p...</td>\n",
       "      <td>Well, there's someone I think might be --</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Can we make this quick?  Roxanne Korrine and A...   \n",
       "1  Not the hacking and gagging and spitting part....   \n",
       "2  You're asking me out.  That's so cute. What's ...   \n",
       "3  No, no, it's my fault -- we didn't have a prop...   \n",
       "4  The thing is, Cameron -- I'm at the mercy of a...   \n",
       "5                                               Why?   \n",
       "6     Gosh, if only we could find Kat a boyfriend...   \n",
       "7                     C'esc ma tete. This is my head   \n",
       "8  I don't want to know how to say that though.  ...   \n",
       "9  How is our little Find the Wench A Date plan p...   \n",
       "\n",
       "                                              answer  \n",
       "0  Well, I thought we'd start with pronunciation,...  \n",
       "1  Okay... then how 'bout we try out some French ...  \n",
       "2                                         Forget it.  \n",
       "3                                           Cameron.  \n",
       "4     Seems like she could get a date easy enough...  \n",
       "5  Unsolved mystery.  She used to be really popul...  \n",
       "6                          Let me see what I can do.  \n",
       "7           Right.  See?  You're ready for the quiz.  \n",
       "8               That's because it's such a nice one.  \n",
       "9          Well, there's someone I think might be --  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_lang[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=3000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~Â©\\\\', lower=True, \n",
    "                                               split=' ', char_level=False, oov_token=None, document_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(questions + answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(questions)\n",
    "Y = tokenizer.texts_to_sequences(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gosh if only we could find a boyfriend'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(X[6], tokenizer.index_word, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_lang.to_csv('data/cornell_dialogue.csv')\n",
    "cornell_seq = [X, Y]\n",
    "pickle.dump(cornell_seq, open('data/cornell_seq.pickle', \"wb\"))\n",
    "pickle.dump(tokenizer, open('data/cornell_tokenizer.pickle', \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
